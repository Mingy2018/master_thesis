## Generative Model - **样本生成模型**

给定数据集$\{x_i\}, i=1,2,3...n$，比如$x_i$是人脸图片，生成模型的概念是学习一个关于$x$的概率分布$P(x)$，然后我们可以从分布中采样，得到新的人脸图片。

机器学习中，一个通用的准则是**最大似然准则**：

**一个对分布较好的估计，应该使得在这个估计下样本集出现的概率最大，即联合概率$P(x_1,x_2,...,x_n)$最大。**

为方便，假设样本集的采样遵循独立同分布，因此联合概率可分解为$P(x_1,x_2,...,x_n)=\prod_{i=1}^n{p(x_i)}$，也被称为**似然函数(likelihood)**。

根据最大似然准则，需要最大化该概率，可以转换为最小化负的取对数，这样做是为了学习训练中求梯度方便：$L(\theta)=-\sum_{i=1}^nlog\ p(x_i;\theta)$

最后在对负对数似然求$\theta$的梯度：$\nabla_{\theta}L(\theta)=-\nabla_{\theta}\sum_{i=1}^nlog\ p(x_i;\theta)=-\sum_{i=1}^nlog\  \nabla_{\theta}\ p(x_i;\theta)$

利用梯度下降法更新$\theta$，让Loss逐渐变小，得到比较理想的分布$P(x)$。

### Variational Autoencoders

VAE is a neural network made up of two parts:

- An *encoder* network that compresses high-dimensional input data into a lower-dimensional representation vector
- An *decoder* network that de-compress a given representation vector back to the origin domain

The network is trained to find weights for the encoder and decoder that minimize the loss between the original input and the reconstruction of the input after it has passed through the encoder and decoder.

#### Latent Variable Models

- *latent variable:* $z$
  - indicates the dimension dependencies of data
  - not exactly known

- a family of deterministic function $f(z;\theta)$ 
  - $\theta$ is the parameters of $f$ function, not the distribution $z$
  - z is latent variable, has the PDF $P(z)$ defined over $Z$
  - $f : Z × Θ → X$

We wish to optimize $\theta$ such that we sample $z$ from $P(z)$ and, with high probability, $f(z;\theta)$ will be like the $X's$ in the dataset.

More precisely, we are aiming maximizing the probability of each $X$ in dataset:

$P(X)=\int P(X|z;\theta)P(z)dz$ 

We often choose the output distribution as Gaussian: $P(X|z; θ) = N (X| f(z; θ), σ^2 ∗ I)$, that is, it has mean $f(z;\theta)$ and covariance $σ^2 ∗ I$.

- standard VAE model

  ![](https://raw.githubusercontent.com/Mingy2018/Markdown-photoes/master/img/20200819214017.png)

#### Two problems of VAE

The aim of VAE is maximizing $P(X)=\int P(X|z;\theta)P(z)dz$ , which has two problems

1. how to define the latent variable $z$

VAE assume the $z$  has simple normal distribution. The key is to notice that any distribution in $d$ dimensions can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function

1. how to deal with the integral over $z$





#### The Loss Function

*Kullback-Leibler (KL) divergence(KL散度):*  $D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p_{x_i}}{q_{x_i}})$

- 定量地确定哪种已有分布能更好地解释真实分布: ask which distribution preserves the most information from our original data source
- help to measure how much information we lose when we choose an approximation
- $p$ is our probability distribution, $q$ is the approximating distribution
- KL Divergence is **not a distance metric**, because KL Divergence is not *symmetric*
- 



*Information Entropy:* $H=-\sum_{i=1}^Np(x_i)\cdot logp(x_i)$

- [ ] the minimum number of bits it would take us to encode our distribution

- theoretical lower bound of needed bits

  

### Generative Adversarial Networks





